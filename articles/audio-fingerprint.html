<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>「MIR-03」听歌识曲: 音乐宇宙里的占星师 - 无痛入门音乐科技</title>
    <style>
      :root {
        --bg: #f5f5f7;
        --card-bg: #ffffff;
        --text: #1d1d1f;
        --text-secondary: #6e6e73;
        --accent: #5856d6;
        --border: #d2d2d7;
        --code-bg: #f0f0f3;
      }
      @media (prefers-color-scheme: dark) {
        :root {
          --bg: #0d0d12;
          --card-bg: #1c1c24;
          --text: #f0f0f5;
          --text-secondary: #8e8e93;
          --accent: #7b79ff;
          --border: #2c2c34;
          --code-bg: #15151d;
        }
      }
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        font-family:
          -apple-system, BlinkMacSystemFont, "SF Pro Display", "Segoe UI",
          Roboto, "Noto Sans SC", sans-serif;
        background: var(--bg);
        color: var(--text);
        line-height: 1.8;
      }
      .header {
        background: linear-gradient(
          135deg,
          #0f0c29 0%,
          #302b63 50%,
          #24243e 100%
        );
        padding: 2rem 1.5rem;
        text-align: center;
      }
      .header a {
        color: rgba(255, 255, 255, 0.7);
        text-decoration: none;
        font-size: 0.9rem;
      }
      .header a:hover {
        color: #fff;
      }
      article {
        max-width: 720px;
        margin: 0 auto;
        padding: 2rem 1.5rem 4rem;
      }
      article h1 {
        font-size: 1.8rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        line-height: 1.3;
      }
      .meta {
        color: var(--text-secondary);
        font-size: 0.85rem;
        margin-bottom: 2rem;
        padding-bottom: 1rem;
        border-bottom: 1px solid var(--border);
      }
      .content {
        font-size: 1rem;
      }
      .content p,
      .content section {
        margin-bottom: 1rem;
      }
      .content img {
        max-width: 100%;
        height: auto;
        border-radius: 8px;
        margin: 1rem 0;
      }
      .content h2,
      .content h3 {
        text-align: center;
        margin: 1.5rem 0 0.8rem;
        font-weight: 600;
      }
      .content blockquote {
        border-left: 3px solid var(--accent);
        padding-left: 1rem;
        color: var(--text-secondary);
        margin: 1rem 0;
      }
      .content a {
        color: var(--accent);
      }
      .content hr {
        border: none;
        border-top: 1px solid var(--border);
        margin: 1.5rem 0;
      }
      .content ul,
      .content ol {
        padding-left: 1.5rem;
        margin-bottom: 1rem;
      }
      .content li {
        margin-bottom: 0.3rem;
      }
      .content table {
        width: 100%;
        border-collapse: collapse;
        margin: 1rem 0;
      }
      .content th,
      .content td {
        border: 1px solid var(--border);
        padding: 0.5rem;
        text-align: left;
      }
      .content pre,
      .content code {
        font-family:
          "SF Mono", SFMono-Regular, Menlo, Consolas, "Liberation Mono",
          monospace;
        font-size: 0.85em;
      }
      .content code {
        background: var(--code-bg);
        border: 1px solid var(--border);
        border-radius: 4px;
        padding: 0.15rem 0.4rem;
      }
      .content pre {
        background: var(--code-bg);
        border: 1px solid var(--border);
        border-radius: 8px;
        padding: 1rem 1.2rem;
        overflow-x: auto;
        margin: 1rem 0;
        line-height: 1.5;
      }
      .content pre code {
        background: none;
        border: none;
        padding: 0;
        font-size: inherit;
        display: block;
      }
      .content pre ol {
        list-style: none;
        padding: 0;
        margin: 0;
      }
      .content pre li {
        margin: 0;
        padding: 0;
      }
      .content pre p {
        margin: 0;
      }
    </style>
  </head>
  <body>
    <div class="header">
      <a href="../index.html">&larr; 返回 无痛入门音乐科技</a>
    </div>
    <article>
      <h1>「MIR-03」听歌识曲: 音乐宇宙里的占星师</h1>
      <div class="meta">2020年3月7日 17:00 · 无痛入门音乐科技</div>
      <div class="content">
        <blockquote>
          <p>
            听歌识曲可以让"求BGM名"不再是个问题, 这背后的算法,
            就如同一个看遍音乐宇宙星图的占星大师傅, 自动为你求解!<br />
          </p>
        </blockquote>
        <p>
          <span><strong>♬ 本文为MIR音乐信息检索系列的第7篇文章 ♬</strong></span>
        </p>
        <p>
          <span
            >先用我刚刚录的30秒视频,
            演示下QQ音乐App里的听音识曲功能:&nbsp;</span
          >
        </p>
        <p>
          <span
            >可见3秒就能把音频识别出来, 不仅返回歌曲基础信息,
            更是直接定位到当前播放的时间戳.&nbsp;我的好友&amp;同事鲁霄大兄弟,
            已经通过QQ音乐的知乎账号发表了4篇技术系列文章,&nbsp;并在MIREX&nbsp;2019
            Audio Fingerprinting竞赛中取得了第一名!&nbsp;所以我这篇更偏科普,
            争取让大家无痛入门~</span
          ><br />
        </p>
        <p>
          <span
            >之所以说整个过程像占星,&nbsp;是因为给定音频片段的指纹与整个曲库的指纹在匹配过程中,
            好比在星图中认出一个星座, 而音频指纹与星图在地球上空的样子,
            更是有异曲同工之妙.</span
          ><br />
        </p>
        <p>
          <img
            data-original-style="width: 100%;height: auto;"
            data-index="2"
            src="images/aeff89ec412b.png"
            _width="100%"
            alt="Image"
            data-report-img-idx="0"
          />
        </p>
        <p>
          <span
            >接下来, 我将回答以下三个问题,
            带大家一步步地了解听歌识曲的原理：</span
          >
        </p>
        <ul>
          <li>
            <p><span>什么是音频指纹?</span></p>
          </li>
          <li>
            <p><span>如何从音频指纹中获得Hashes?</span></p>
          </li>
          <li>
            <p><span>如何用Hashes进行歌曲匹配？</span></p>
          </li>
        </ul>
        <hr />
        <h3>『音频指纹』</h3>
        <p>
          <span
            >音频指纹就如人的指纹一样,&nbsp;可以有效地代表一首歌.&nbsp;因为听歌识曲做匹配时,
            是和曲库千万级歌曲做比对, 那么音频指纹就要足够<strong>精简</strong>,
            才能3秒内就算出匹配结果;
            同时,&nbsp;音频指纹还要有足够的<strong>辨识度</strong>,&nbsp;比如在"求问背景音乐是什么"这个场景下,&nbsp;不可避免地有其他音频信号掺杂其中(比如男女主在说话),
            那背景音乐的音频指纹如何在干扰环境下依然脱颖而出?</span
          >
        </p>
        <p>
          <span
            >那让我们仰望上图的星空,&nbsp;看到由中间几颗星星辨识出的天蝎座,
            其实星座本身不止有这些星星,
            只是它们总是最亮的那几颗,&nbsp;亮到抗得住地球天空状态的干扰. 同理,
            如果能挑出音频里最亮的星星, 构成其指纹, 不就妥妥的了?<br
          /></span>
        </p>
        <p>
          <span
            >业界第一个开发音频指纹的公司Shazam, 也是这么想的:
            挑出"音频波形进行STFT变换后得到的spectrogram"里高能量的峰点们,
            构成constellation map星云图, 也就是这个音频的指纹模样了!</span
          >
        </p>
        <p>
          <img
            data-original-style="width: 100%;height: auto;"
            data-index="3"
            src="images/405c22b9d95f.png"
            _width="100%"
            alt="Image"
          />
        </p>
        <p>
          <span
            ><span>☞&nbsp;</span>不记得spectrogram是什么了? 回看<a
              target="_blank"
              href="http://mp.weixin.qq.com/s?__biz=MzU5MzY3NzI0OA==&amp;mid=2247483684&amp;idx=1&amp;sn=d3f3746a6837f7d12044d1931cfd4ead&amp;chksm=fe0d9f8bc97a169dbfcb80443e8da87d8e35ae643bae76841060442d8f88a4f292cda594afa5&amp;scene=21#wechat_redirect"
              data-itemshowtype="0"
              tab="innerlink"
              data-linktype="2"
              >「MIR-01」要把音乐画出来，总共分几步？</a
            ></span
          ><br />
        </p>
        <p>
          <span
            >至此算是给出了解决"辨识度"的方案,
            那如何用更精简有效的数据格式表示星云图?&nbsp;答案是<strong>Hashes</strong>.</span
          >
        </p>
        <hr />
        <h3>『Hashes』</h3>
        <p>
          <span
            >最最简单的表示方式, 莫过于直接用 [(4s, 800Hz), (7s, 1000Hz), (10s,
            1700Hz)]
            这样的方式表示图1B中的点,&nbsp;然而问题是,&nbsp;4s/7s/10s只是当前这个音频片段a里的时间信息,&nbsp;但这个音频片段很可能出自于某首歌A里的第44s/47s/50s,&nbsp;拿着这种形式的时间信息去做匹配并没有什么用...</span
          >
        </p>
        <p>
          <span
            >更何况, 听歌识曲实现的是,&nbsp;听一丢丢片段, 就能知道是哪首歌.
            利用<span>[</span><span>(</span><span>4s, 800Hz</span><span>)</span
            ><span>, (7s, 1000</span><span>Hz</span><span>)</span><span>,</span
            ><span> (10s, 1700Hz</span><span>)</span
            ><span>]&nbsp;</span>这样的数据, 算法就只能从头到尾听完整首歌,
            才能识别出来了.</span
          >
        </p>
        <p>
          <span
            >因此,&nbsp;相比于用绝对的时间和频率的值,
            不如用<strong>相对</strong>的数据:&nbsp;将一个点(t1, f1),
            与其之后的一个点(t2, f2), 这一对表示为由3个数值构成的一个<strong
              >hash = f1: f2:&nbsp;t2-t1&nbsp;</strong
            ></span
          ><span
            >这样把音频里所有一对对存储成hashes,&nbsp;在音乐意义上就是存储了当前音频中,
            能被明显听到的音符们, 在发生音程变化时所用的时间.</span
          >
        </p>
        <p>
          <img
            data-original-style="width: 100%;height: auto;"
            data-index="4"
            src="images/84512cc0232e.png"
            _width="100%"
            alt="Image"
          />
        </p>
        <p><span></span></p>
        <hr />
        <h3>『匹配流程』</h3>
        <p>
          <span
            >当我们拥有曲库所有歌曲各自的hashes,&nbsp;你想让App识别的歌曲片段a的hashes也被计算出来之后,
            匹配过程无非是:&nbsp;当hashes(a)与曲库中一首歌A的hashes(A)呈现出最大数量的匹配hash,
            那么这个歌曲片段a很有可能来自于歌曲A.</span
          >
        </p>
        <p>
          <span
            >但是问题又来了!&nbsp;"最大数量的匹配hash"没有考虑hash的先后次序,&nbsp;仅仅通过这个条件判定识别结果会有什么后果?&nbsp;比如歌曲片段a听上去应该是小星星的旋律:&nbsp;C
            C G G&nbsp;A A G 且音符之间均隔0.5秒;
            这时曲库里有一首歌B,&nbsp;在开头2秒出现了A A G, 中间副歌100秒时出现C
            C G G, 最后结束的2秒出现了G A.
            这种情况时,&nbsp;hashes(a)也都被包含在hashes(B)里呈现最大数量的匹配,
            但明显不能说片段a来自于歌曲B呀!<br
          /></span>
        </p>
        <p>
          <span
            >解决办法是,
            当<span>hashes(a)</span>有个hash匹配hashes(A)的一个hash时,
            记录该hash分别在片段a和歌曲A中的时间戳, 以下图为例:&nbsp;</span
          >
        </p>
        <p>
          <img
            data-original-style=""
            data-index="5"
            src="images/ab3cab3f0ca9.png"
            _width="677px"
            alt="Image"
          />
        </p>
        <p>
          <span
            >上图中, 歌曲片段a (sample soundfile) 共25秒,&nbsp;曲库里的一首歌曲A
            (database soundfile) 共100秒, 在其第40秒到60多秒之间,
            有很多和hashes(a)的匹配, 这里每一个匹配都用一个蓝圆圈表示.<br
          /></span>
        </p>
        <p>
          <span
            >若将圆圈对应的database soundfile time减去sample soundfile
            time,&nbsp;可得大部分时间差为40s,
            具体时间差得出的直方图分布如下所示:</span
          >
        </p>
        <p>
          <img
            data-original-style="width: 100%;height: auto;"
            data-index="6"
            src="images/a18202572b50.png"
            _width="100%"
            alt="Image"
          />
        </p>
        <p>
          <span
            >可以推断<strong
              >片段a应该就是歌曲A中第40秒到第65秒的片段,
              听歌识曲结果返回歌曲A!&nbsp;</strong
            ></span
          ><span>如果是片段a和反例中的</span><span>曲</span><span>库歌曲</span
          ><span>B做匹配</span><span>, </span><span>那</span
          ><span>上面同样的两</span><span>个图示</span><span>就</span
          ><span>会呈现出</span><span>:</span>
        </p>
        <p>
          <img
            data-original-style="width: 100%;height: auto;"
            data-index="7"
            src="images/9a4581bf3f87.png"
            _width="100%"
            alt="Image"
          />
        </p>
        <p>
          <span
            >因此在数据库里真正存储歌曲的hashes时,
            还需要存上每条hash在该歌曲中的时间戳 (</span
          ><strong
            >hash: time = [<strong>f1: f2:&nbsp;t2-t1</strong>]: t1</strong
          ><span>)</span>
        </p>
        <hr />
        <p>
          <span
            >通过以上的科普,
            希望大家对听歌识曲的基本原理有了大概理解,&nbsp;其实还有许许多多科普文章里说不清楚的技术细节...&nbsp;</span
          >
        </p>
        <p>
          <span
            >广告时间: 欢迎大家体验QQ音乐App里听歌识曲的功能,
            另有专门的"<strong>Q音探歌</strong>"App请多支持!&nbsp;</span
          >
        </p>
        <p>
          <span
            ><strong><span>✎ 文中图例来自文献</span></strong></span
          ><strong><span>：</span></strong
          ><span
            >Avery Li-Chun Wang. "An industrial strength audio search
            algorithm." In&nbsp;ISMIR, pp. 7-13. 2003.</span
          >
        </p>
      </div>
    </article>
  </body>
</html>
