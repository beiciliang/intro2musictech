# 超越基于文本的交互

在本教程的最后，有一个在文本生成音乐（TTM）讨论中越来越频繁出现的核心问题：我们*真的需要文本*作为音乐生成的控制方式吗？这个问题常常伴随着一句广为流传但出处不详的名言：

> *"Writing about Music is like Dancing about Architecture"*
>（用文字描述音乐，就像用舞蹈描述建筑一样）

更正式地说，文本作为控制媒介的主要不足在于其缺乏具体性以及无法表达细粒度的、对音乐至关重要的细节：

1. **低具体性**：由于大多数文本描述数据集通常是从元数据中提取的，而元数据本身仅包含体裁、情绪、功能以及最多乐器级别的标签等高层次信息，因此从文本到音乐的映射总体上是一个强一对多的函数（作为练习，可以想象有多少首歌曲可能符合"激动人心、节奏轻快的摇滚音乐，带有鼓和吉他"这样的描述）。这意味着 TTM 模型很少能学会在体裁级别以上的粒度跟随文本，因为在给定的高层模式中，所有文本描述在音乐内容上都非常相似。
2. **无法表达细粒度细节**：虽然文本擅长描述高层信息甚至乐器类别，但其整体分辨率相当粗糙，难以描述在高分辨率下变化的细粒度特征。这对音乐来说尤其不利，因为许多音乐控制参数（音量、旋律、和弦、节奏）都需要精细的时间分辨率才能准确描述和控制。
3. **与音乐实际使用场景不匹配**：虽然文本生成音乐旨在颠覆当前的创作工作流程并创造新的艺术表达途径，但文本输入并不特别契合许多音乐人的工作流程和使用场景。音乐人往往更直接地在音频层面进行操作（即给定一些音频，创作其他音频），他们可能不需要完整的文本到歌曲系统（而更倾向于循环器、采样生成器、伴奏系统以及可以实时运行的工具）。

既然如此，我们接下来该何去何从？幸运的是，MIR 社区中越来越多的研究正在为这些最先进的音频域音乐生成系统探索更定制化的控制方式。我们将介绍几个大的类别，并特别强调：既有*基于训练的*控制方法（即构建新的音乐生成系统），也有*无需训练的*控制方法（即无需额外训练就能为 TTM 系统赋予新的控制能力）。


## 音频到音频控制

也许最直觉（且计算上最简单）的控制方式就是让音乐以其他音乐为条件！这可以有多种形式，例如：

- 音乐伴奏生成（例如 鼓 -> 鼓 + 吉他）
- 人声伴奏生成
- 人声变形 / 音色迁移（即用人声控制调制音乐）
- 迭代编辑（即给定已有的音乐输出，基于原始音频对其进行*修改*）

在基于训练的方面，许多伴奏类工作如 MSDM {cite}`mariani2023multi`、SingSong {cite}`donahue2023singsong`、StemGen {cite}`parker2024stemgen` 和 Diff-a-Riff {cite}`nistal2024diff` 从零开始训练模型，直接建模条件音频到音频（或跨音轨的联合）分布，使参考音频通过我们已经见过的许多方式（特别是交叉注意力和通道级拼接）来影响生成过程。即使是较早的 RAVE 模型 {cite}`caillon2021rave` 也是基于训练的音频到音频模型考虑实际需求的好例子，其高度优化的设计支持实时人声变形和音色迁移。

在无需训练的方面，许多音频到音频的工作聚焦于利用 TTM 模型进行无需训练的*编辑*，因为现有的 TTM 可以作为相当强大的生成先验。这些方法从较简单的 VampNet 风格的周期性修补 {cite}`garcia2023vampnet` 和 Stable Audio Open {cite}`evans2024open` 的初始音频风格迁移——它们只是遮盖音频的某些区域并用模型重新生成，利用 TTM 系统的先验进行创意编辑——到更定制化的基于引导/梯度的编辑方法，如 DDPM Inversion {cite}`Manor2024ZeroShotUA`、MusicMagus {cite}`zhang2024musicmagus` 和 DITTO-(1/2) {cite}`novack2024ditto,Novack2024DITTO2DD`，它们利用可逆性或通过生成过程的可微分性来改变模型的输出。值得注意的是，这正是基于扩散模型的方法明显优于基于语言模型方法的领域，因为语言模型生成的离散特性使得这些推理时的技巧非常不稳定且难以使用。


## 抽象音乐控制

对音乐人来说更有趣的是真正突破*任何*特定的音频或文本框架，允许完全自定义的控制机制——这些机制与音乐实践相契合，但可能难以用文字描述。例如：

- 随时间变化的音乐强度（即音量以及和声和节奏密度）
- 随时间变化的旋律
- 随时间变化的和弦
- 整体音乐结构（例如主歌、副歌）

正如我们已经多次看到的，基于训练的方法如 Music ControlNet {cite}`wu2023music`、JASCO {cite}`Tal2024JointAA` 和 Mustango {cite}`melechovsky2023mustango` 可以通过定义某种嵌入函数来提取控制的潜在向量，并通过标准方式让它们调制音频生成过程来输入控制信号。此外，无需训练的方法如 DITTO {cite}`novack2024ditto,Novack2024DITTO2DD`、SMITIN {cite}`koo2024smitin` 和 Guidance Gradients {cite}`Levy2023ControllableMP` 可以利用类似的采样可微分性来引导生成过程。
