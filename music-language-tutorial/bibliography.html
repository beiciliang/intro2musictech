
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Bibliography &#8212; 连接音乐音频与自然语言</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'bibliography';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="超越基于文本的交互" href="conclusion/beyondtext.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="连接音乐音频与自然语言 - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="连接音乐音频与自然语言 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    连接音乐音频与自然语言
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">第一章 引言</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction/background.html">背景</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction/overview.html">教程概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction/advantange.html">为什么选择自然语言？</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">第二章 语言模型概述</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lm/intro.html">简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="lm/framework.html">框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="lm/advances.html">研究进展</a></li>
<li class="toctree-l1"><a class="reference internal" href="lm/challenges.html">挑战</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">第三章 音乐描述</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="description/intro.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="description/tasks.html">任务</a></li>
<li class="toctree-l1"><a class="reference internal" href="description/models.html">模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="description/datasets.html">数据集</a></li>
<li class="toctree-l1"><a class="reference internal" href="description/evaluation.html">评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="description/code.html">代码实践</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">第四章 文本到音乐检索</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="retrieval/intro.html">简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="retrieval/models.html">模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="retrieval/evaluate.html">评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="retrieval/code.html">代码实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="retrieval/challenge.html">挑战</a></li>
<li class="toctree-l1"><a class="reference internal" href="retrieval/conversational_retrieval.html">对话式检索</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">第五章 文本到音乐生成</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="generation/intro.html">简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="generation/evaluation.html">评估</a></li>
<li class="toctree-l1"><a class="reference internal" href="generation/lmmodel.html">MusicGEN</a></li>
<li class="toctree-l1"><a class="reference internal" href="generation/diffusionmodel.html">基于 Diffusion Model 的文本到音乐生成</a></li>
<li class="toctree-l1"><a class="reference internal" href="generation/code.html">代码教程</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">第六章 总结</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="conclusion/intro.html">总结</a></li>
<li class="toctree-l1"><a class="reference internal" href="conclusion/beyondaudio.html">超越音频模态</a></li>
<li class="toctree-l1"><a class="reference internal" href="conclusion/beyondtext.html">超越基于文本的交互</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">参考文献</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/beiciliang/intro2musictech" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/beiciliang/intro2musictech/issues/new?title=Issue%20on%20page%20%2Fbibliography.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/bibliography.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bibliography</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bibliography">
<h1>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading">#</a></h1>
<div class="docutils container" id="id1">
<div role="list" class="citation-list">
<div class="citation" id="id292" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>rif<span class="fn-bracket">]</span></span>
<p>Riffusion. <a class="reference external" href="https://www.riffusion.com">https://www.riffusion.com</a>.</p>
</div>
<div class="citation" id="id294" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>sun<span class="fn-bracket">]</span></span>
<p>Riffusion. <a class="reference external" href="https://suno.com">https://suno.com</a>.</p>
</div>
<div class="citation" id="id293" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>udi<span class="fn-bracket">]</span></span>
<p>Udio. <a class="reference external" href="https://www.udio.com">https://www.udio.com</a>.</p>
</div>
<div class="citation" id="id17" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>AAA+23<span class="fn-bracket">]</span></span>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and others. Gpt-4 technical report. <em>arXiv preprint arXiv:2303.08774</em>, 2023.</p>
</div>
<div class="citation" id="id303" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ADB+23<span class="fn-bracket">]</span></span>
<p>Andrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, and others. Musiclm: generating music from text. <em>arXiv preprint arXiv:2301.11325</em>, 2023.</p>
</div>
<div class="citation" id="id171" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>And82<span class="fn-bracket">]</span></span>
<p>Brian DO Anderson. Reverse-time diffusion equation models. <em>Stochastic Processes and their Applications</em>, 12(3):313–326, 1982.</p>
</div>
<div class="citation" id="id103" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CE21<span class="fn-bracket">]</span></span>
<p>Antoine Caillon and Philippe Esling. RAVE: a variational autoencoder for fast and high-quality neural audio synthesis. <em>arXiv:2111.05011</em>, 2021.</p>
</div>
<div class="citation" id="id2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CLZ+23<span class="fn-bracket">]</span></span>
<p>Arun Tejasvi Chaganty, Megan Leszczynski, Shu Zhang, Ravi Ganti, Krisztian Balog, and Filip Radlinski. Beyond single items: exploring user preferences in item sets with the conversational playlist curation dataset. In <em>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 2023.</p>
</div>
<div class="citation" id="id283" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CZJ+22<span class="fn-bracket">]</span></span>
<p>Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: masked generative image transformer. In <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR</em>, 11305–11315. IEEE, 2022. URL: <a class="reference external" href="https://doi.org/10.1109/CVPR52688.2022.01103">https://doi.org/10.1109/CVPR52688.2022.01103</a>.</p>
</div>
<div class="citation" id="id280" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CWBergKirkpatrickD20<span class="fn-bracket">]</span></span>
<p>Ke Chen, Cheng-i Wang, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Music sketchnet: controllable music generation via factorized representations of pitch and rhythm. In <em>Proceedings of the 21th International Society for Music Information Retrieval Conference, ISMIR</em>, 77–84. 2020.</p>
</div>
<div class="citation" id="id79" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CWL+24<span class="fn-bracket">]</span></span>
<p>Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. MusicLDM: enhancing novelty in text-to-music generation using beat-synchronous mixup strategies. In <em>IEEE International Conference on Audio, Speech and Signal Processing (ICASSP)</em>. 2024.</p>
</div>
<div class="citation" id="id20" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CXZ+22<span class="fn-bracket">]</span></span>
<p>Tianyu Chen, Yuan Xie, Shuai Zhang, Shaohan Huang, Haoyi Zhou, and Jianxin Li. Learning music sequence representation from text supervision. In <em>ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 4583–4587. IEEE, 2022.</p>
</div>
<div class="citation" id="id15" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CLPN19<span class="fn-bracket">]</span></span>
<p>Jeong Choi, Jongpil Lee, Jiyoung Park, and Juhan Nam. Zero-shot learning for audio-based music classification and tagging. In <em>ISMIR</em>. 2019.</p>
</div>
<div class="citation" id="id48" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CFS16<span class="fn-bracket">]</span></span>
<p>Keunwoo Choi, George Fazekas, and Mark Sandler. Towards music captioning: generating music playlist descriptions. In <em>Extended abstracts for the Late-Breaking Demo Session of the 17th International Society for Music Information Retrieval Conference</em>. 08 2016. <a class="reference external" href="https://doi.org/10.48550/arXiv.1608.04868">doi:10.48550/arXiv.1608.04868</a>.</p>
</div>
<div class="citation" id="id49" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CFSC17<span class="fn-bracket">]</span></span>
<p>Keunwoo Choi, György Fazekas, Mark Sandler, and Kyunghyun Cho. Convolutional recurrent neural networks for music classification. In <em>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, volume, 2392–2396. 2017. <a class="reference external" href="https://doi.org/10.1109/ICASSP.2017.7952585">doi:10.1109/ICASSP.2017.7952585</a>.</p>
</div>
<div class="citation" id="id62" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CXZ+23<span class="fn-bracket">]</span></span>
<p>Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models. December 2023. arXiv:2311.07919 [cs, eess]. URL: <a class="reference external" href="http://arxiv.org/abs/2311.07919">http://arxiv.org/abs/2311.07919</a> (visited on 2024-02-26), <a class="reference external" href="https://doi.org/10.48550/arXiv.2311.07919">doi:10.48550/arXiv.2311.07919</a>.</p>
</div>
<div class="citation" id="id12" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CHL+24<span class="fn-bracket">]</span></span>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, and others. Scaling instruction-finetuned language models. <em>Journal of Machine Learning Research</em>, 25(70):1–53, 2024.</p>
</div>
<div class="citation" id="id19" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CKG+24<span class="fn-bracket">]</span></span>
<p>Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez. Simple and controllable music generation. <em>Advances in Neural Information Processing Systems</em>, 2024.</p>
</div>
<div class="citation" id="id57" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DML+24<span class="fn-bracket">]</span></span>
<p>Zihao Deng, Yinghao Ma, Yudong Liu, Rongchen Guo, Ge Zhang, Wenhu Chen, Wenhao Huang, and Emmanouil Benetos. MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, <em>Findings of the Association for Computational Linguistics: NAACL 2024</em>, 3643–3655. Mexico City, Mexico, June 2024. Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/2024.findings-naacl.231">https://aclanthology.org/2024.findings-naacl.231</a> (visited on 2024-07-04).</p>
</div>
<div class="citation" id="id61" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DESW23<span class="fn-bracket">]</span></span>
<p>Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. Pengi: An Audio Language Model for Audio Tasks. In <em>Thirty-seventh Conference on Neural Information Processing Systems</em>. 2023. arXiv:2305.11834 [cs, eess]. URL: <a class="reference external" href="http://arxiv.org/abs/2305.11834">http://arxiv.org/abs/2305.11834</a> (visited on 2024-02-16), <a class="reference external" href="https://doi.org/10.48550/arXiv.2305.11834">doi:10.48550/arXiv.2305.11834</a>.</p>
</div>
<div class="citation" id="id33" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DCLT18<span class="fn-bracket">]</span></span>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>, 2018.</p>
</div>
<div class="citation" id="id27" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DJP+20a<span class="fn-bracket">]</span></span>
<p>Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: a generative model for music. <em>arXiv preprint arXiv:2005.00341</em>, 2020.</p>
</div>
<div class="citation" id="id289" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DJP+20b<span class="fn-bracket">]</span></span>
<p>Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music. <em>CoRR</em>, 2020.</p>
</div>
<div class="citation" id="id172" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DN21<span class="fn-bracket">]</span></span>
<p>Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. <em>Neural Information Processing Systems (NeurIPS)</em>, 2021.</p>
</div>
<div class="citation" id="id4" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DCK+24<span class="fn-bracket">]</span></span>
<p>SeungHeon Doh, Keunwoo Choi, Daeyong Kwon, Taesu Kim, and Juhan Nam. Music discovery dialogue generation using human intent analysis and large language models. <em>arXiv preprint arXiv:2411.07439</em>, 2024.</p>
</div>
<div class="citation" id="id24" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DCLN23<span class="fn-bracket">]</span></span>
<p>SeungHeon Doh, Keunwoo Choi, Jongpil Lee, and Juhan Nam. Lp-musiccaps: llm-based pseudo music captioning. In <em>International Society for Music Information Retrieval (ISMIR)</em>. 2023.</p>
</div>
<div class="citation" id="id51" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DLJN24<span class="fn-bracket">]</span></span>
<p>SeungHeon Doh, Jongpil Lee, Dasaem Jeong, and Juhan Nam. Musical word embedding for music tagging and retrieval. <em>arXiv preprint arXiv:2404.13569</em>, 2024.</p>
</div>
<div class="citation" id="id21" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DWCN23<span class="fn-bracket">]</span></span>
<p>SeungHeon Doh, Minz Won, Keunwoo Choi, and Juhan Nam. Toward universal text-to-music retrieval. In <em>ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 1–5. IEEE, 2023.</p>
</div>
<div class="citation" id="id117" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DCR+23<span class="fn-bracket">]</span></span>
<p>Chris Donahue, Antoine Caillon, Adam Roberts, Ethan Manilow, Philippe Esling, Andrea Agostinelli, Mauro Verzetti, and others. SingSong: generating musical accompaniments from singing. <em>arXiv:2301.12662</em>, 2023.</p>
</div>
<div class="citation" id="id126" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DMP19<span class="fn-bracket">]</span></span>
<p>Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. In <em>International Conference on Learning Representations (ICLR)</em>. 2019.</p>
</div>
<div class="citation" id="id282" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DCD+23<span class="fn-bracket">]</span></span>
<p>Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian J. McAuley, and Taylor Berg-Kirkpatrick. Multitrack music transformer. In <em>IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP</em>, 1–5. IEEE, 2023.</p>
</div>
<div class="citation" id="id276" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DHYY18<span class="fn-bracket">]</span></span>
<p>Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. Musegan: multi-track sequential generative adversarial networks for symbolic music generation and accompaniment. In <em>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</em>, 34–41. AAAI Press, 2018.</p>
</div>
<div class="citation" id="id301" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DefossezCSA23<span class="fn-bracket">]</span></span>
<p>Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. <em>Trans. Mach. Learn. Res.</em>, 2023.</p>
</div>
<div class="citation" id="id66" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DCSA22<span class="fn-bracket">]</span></span>
<p>Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. <em>arXiv:2210.13438</em>, 2022.</p>
</div>
<div class="citation" id="id40" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ELBMG07<span class="fn-bracket">]</span></span>
<p>Douglas Eck, Paul Lamere, Thierry Bertin-Mahieux, and Stephen Green. Automatic generation of social tags for music recommendation. <em>Advances in neural information processing systems</em>, 2007.</p>
</div>
<div class="citation" id="id104" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EHGR20<span class="fn-bracket">]</span></span>
<p>Jesse Engel, Lamtharn Hantrakul, Chenjie Gu, and Adam Roberts. DDSP: differentiable digital signal processing. In <em>International Conference on Learning Representations (ICLR)</em>. 2020.</p>
</div>
<div class="citation" id="id35" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ERR+17a<span class="fn-bracket">]</span></span>
<p>Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Mohammad Norouzi, Douglas Eck, and Karen Simonyan. Neural audio synthesis of musical notes with wavenet autoencoders. In <em>International Conference on Machine Learning</em>. PMLR, 2017.</p>
</div>
<div class="citation" id="id288" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ERR+17b<span class="fn-bracket">]</span></span>
<p>Jesse H. Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Mohammad Norouzi, Douglas Eck, and Karen Simonyan. Neural audio synthesis of musical notes with wavenet autoencoders. In <em>Proceedings of the 34th International Conference on Machine Learning, ICML</em>, volume 70 of Proceedings of Machine Learning Research, 1068–1077. PMLR, 2017.</p>
</div>
<div class="citation" id="id154" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ECT+24<span class="fn-bracket">]</span></span>
<p>Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. <em>International Conference on Machine Learning (ICML)</em>, 2024.</p>
</div>
<div class="citation" id="id156" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EPC+24<span class="fn-bracket">]</span></span>
<p>Zach Evans, Julian D Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. <em>arXiv:2407.14358</em>, 2024.</p>
</div>
<div class="citation" id="id71" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>FM22<span class="fn-bracket">]</span></span>
<p>Seth Forsgren and Hayk Martiros. Riffusion: Stable diffusion for real-time music generation. 2022. URL: <a class="reference external" href="https://riffusion.com/about">https://riffusion.com/about</a>.</p>
</div>
<div class="citation" id="id38" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>FLTZ10<span class="fn-bracket">]</span></span>
<p>Zhouyu Fu, Guojun Lu, Kai Ming Ting, and Dengsheng Zhang. A survey of audio-based music classification and annotation. <em>IEEE transactions on multimedia</em>, 2010.</p>
</div>
<div class="citation" id="id46" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GHE22<span class="fn-bracket">]</span></span>
<p>Giovanni Gabbolini, Romain Hennequin, and Elena Epure. Data-efficient playlist captioning with musical and linguistic knowledge. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, <em>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, 11401–11415. Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/2022.emnlp-main.784">https://aclanthology.org/2022.emnlp-main.784</a>, <a class="reference external" href="https://doi.org/10.18653/v1/2022.emnlp-main.784">doi:10.18653/v1/2022.emnlp-main.784</a>.</p>
</div>
<div class="citation" id="id64" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GLTQ23<span class="fn-bracket">]</span></span>
<p>Wenhao Gao, Xiaobing Li, Yun Tie, and Lin Qi. Music Question Answering Based on Aesthetic Experience. In <em>2023 International Joint Conference on Neural Networks (IJCNN)</em>, 01–06. June 2023. ISSN: 2161-4407. URL: <a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/10191775">https://ieeexplore.ieee.org/abstract/document/10191775</a> (visited on 2024-03-01), <a class="reference external" href="https://doi.org/10.1109/IJCNN54540.2023.10191775">doi:10.1109/IJCNN54540.2023.10191775</a>.</p>
</div>
<div class="citation" id="id118" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GSKP23<span class="fn-bracket">]</span></span>
<p>Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, and Bryan Pardo. VampNet: music generation via masked acoustic token modeling. In <em>International Society for Music Information Retrieval (ISMIR)</em>. 2023.</p>
</div>
<div class="citation" id="id23" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GDSB23<span class="fn-bracket">]</span></span>
<p>Josh Gardner, Simon Durand, Daniel Stoller, and Rachel M Bittner. Llark: a multimodal foundation model for music. <em>arXiv preprint arXiv:2310.07160</em>, 2023.</p>
</div>
<div class="citation" id="id290" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GGDRe22<span class="fn-bracket">]</span></span>
<p>Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. It's raw! audio generation with state-space models. In <em>International Conference on Machine Learning, ICML</em>, volume 162 of Proceedings of Machine Learning Research, 7616–7633. PMLR, 2022.</p>
</div>
<div class="citation" id="id313" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GLL+23<span class="fn-bracket">]</span></span>
<p>Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlinsky, and James Glass. Listen, think, and understand. <em>arXiv preprint arXiv:2305.10790</em>, 2023.</p>
</div>
<div class="citation" id="id296" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GL83<span class="fn-bracket">]</span></span>
<p>Daniel W. Griffin and Jae S. Lim. Signal estimation from modified short-time fourier transform. In <em>IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP</em>, 804–807. IEEE, 1983.</p>
</div>
<div class="citation" id="id275" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HPN17<span class="fn-bracket">]</span></span>
<p>Gaëtan Hadjeres, François Pachet, and Frank Nielsen. Deepbach: a steerable model for bach chorales generation. In <em>Proceedings of the 34th International Conference on Machine Learning, ICML</em>, volume 70 of Proceedings of Machine Learning Research, 1362–1371. PMLR, 2017.</p>
</div>
<div class="citation" id="id270" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HHL+23<span class="fn-bracket">]</span></span>
<p>Zihao He, Weituo Hao, Wei-Tsung Lu, Changyou Chen, Kristina Lerman, and Xuchen Song. Alcap: alignment-augmented music captioner. In <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, 16501–16512. 2023.</p>
</div>
<div class="citation" id="id298" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HRU+17<span class="fn-bracket">]</span></span>
<p>Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In <em>Neural Information Processing Systems NeurIPS</em>, 6626–6637. 2017.</p>
</div>
<div class="citation" id="id9" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HI79<span class="fn-bracket">]</span></span>
<p>Lejaren Arthur Hiller and Leonard M. Isaacson. <em>Experimental Music; Composition with an Electronic Computer</em>. Greenwood Publishing Group Inc., 1979.</p>
</div>
<div class="citation" id="id175" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HJA20<span class="fn-bracket">]</span></span>
<p>Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. <em>Neural Information Processing Systems (NeurIPS)</em>, 2020.</p>
</div>
<div class="citation" id="id277" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HVU+19<span class="fn-bracket">]</span></span>
<p>Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer: generating music with long-term structure. In <em>International Conference on Learning Representations, ICLR</em>. OpenReview.net, 2019.</p>
</div>
<div class="citation" id="id26" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HJL+22<span class="fn-bracket">]</span></span>
<p>Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel PW Ellis. Mulan: a joint embedding of music audio and natural language. <em>arXiv preprint arXiv:2208.12415</em>, 2022.</p>
</div>
<div class="citation" id="id279" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HY20<span class="fn-bracket">]</span></span>
<p>Yu-Siang Huang and Yi-Hsuan Yang. Pop music transformer: beat-based modeling and generation of expressive pop piano compositions. In <em>The 28th ACM International Conference on Multimedia</em>, 1180–1188. ACM, 2020.</p>
</div>
<div class="citation" id="id59" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HLSS23<span class="fn-bracket">]</span></span>
<p>Atin Sakkeer Hussain, Shansong Liu, Chenshuo Sun, and Ying Shan. M2UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models. <em>arXiv preprint arXiv:2311.11255</em>, 2023.</p>
</div>
<div class="citation" id="id300" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KZRS19<span class="fn-bracket">]</span></span>
<p>Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fréchet audio distance: A reference-free metric for evaluating music enhancement algorithms. In <em>Interspeech</em>, 2350–2354. ISCA, 2019.</p>
</div>
<div class="citation" id="id243" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KLL+23<span class="fn-bracket">]</span></span>
<p>Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: learning probability flow ODE trajectory of diffusion. In <em>International Conference on Learning Representations (ICLR)</em>. 2023.</p>
</div>
<div class="citation" id="id44" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KSM+10<span class="fn-bracket">]</span></span>
<p>Youngmoo E Kim, Erik M Schmidt, Raymond Migneco, Brandon G Morton, Patrick Richardson, Jeffrey Scott, Jacquelin A Speck, and Douglas Turnbull. Music emotion recognition: a state of the art review. In <em>Proc. ismir</em>, volume 86, 937–952. 2010.</p>
</div>
<div class="citation" id="id299" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KCI+20<span class="fn-bracket">]</span></span>
<p>Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D. Plumbley. Panns: large-scale pretrained audio neural networks for audio pattern recognition. <em>IEEE ACM Trans. Audio Speech Lang. Process.</em>, 28:2880–2894, 2020.</p>
</div>
<div class="citation" id="id314" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KGB+24<span class="fn-bracket">]</span></span>
<p>Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro. Audio flamingo: a novel audio language model with few-shot learning and dialogue abilities. In <em>ICML</em>. 2024. URL: <a class="reference external" href="https://openreview.net/forum?id=WYi3WKZjYe">https://openreview.net/forum?id=WYi3WKZjYe</a>.</p>
</div>
<div class="citation" id="id263" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KWG+24<span class="fn-bracket">]</span></span>
<p>Junghyun Koo, Gordon Wichern, Francois G Germain, Sameer Khurana, and Jonathan Le Roux. Smitin: self-monitored inference-time intervention for generative music transformers. <em>arXiv preprint arXiv:2404.02252</em>, 2024.</p>
</div>
<div class="citation" id="id295" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KSP+23<span class="fn-bracket">]</span></span>
<p>Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: textually guided audio generation. In <em>The Eleventh International Conference on Learning Representations, ICLR</em>. OpenReview.net, 2023.</p>
</div>
<div class="citation" id="id151" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KSL+23<span class="fn-bracket">]</span></span>
<p>Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved RVQGAN. In <em>Neural Information Processing Systems (NeurIPS)</em>. 2023.</p>
</div>
<div class="citation" id="id41" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Lam08<span class="fn-bracket">]</span></span>
<p>Paul Lamere. Social tagging and music information retrieval. <em>Journal of new music research</em>, 2008.</p>
</div>
<div class="citation" id="id305" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LPPW24<span class="fn-bracket">]</span></span>
<p>Luca A Lanzendorfer, Nathanal Perraudin, Constantin Pinkl, and Roger Wattenhofer. BLAP: Bootstrapping Language-Audio Pre-training for Music Captioning. In <em>Workshop on AI-Driven Speech, Music, and Sound Generation</em>. 2024.</p>
</div>
<div class="citation" id="id5" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Lee10<span class="fn-bracket">]</span></span>
<p>Jin Ha Lee. Analysis of user needs and information features in natural language queries seeking music information. <em>Journal of the American Society for Information Science and Technology</em>, 61(5):1025–1045, 2010.</p>
</div>
<div class="citation" id="id307" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LL24<span class="fn-bracket">]</span></span>
<p>Jinwoo Lee and Kyogu Lee. Do captioning metrics reflect music semantic alignment? In <em>International Society for Music Information Retrieval (ISMIR) 2024, Late Breaking Demo (LBD)</em>. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2411.11692">https://arxiv.org/abs/2411.11692</a>.</p>
</div>
<div class="citation" id="id50" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LN17<span class="fn-bracket">]</span></span>
<p>Jongpil Lee and Juhan Nam. Multi-level and multi-scale feature aggregation using pretrained convolutional neural networks for music auto-tagging. <em>IEEE Signal Processing Letters</em>, 24(8):1208–1212, 2017. <a class="reference external" href="https://doi.org/10.1109/LSP.2017.2713830">doi:10.1109/LSP.2017.2713830</a>.</p>
</div>
<div class="citation" id="id3" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LZG+23<span class="fn-bracket">]</span></span>
<p>Megan Leszczynski, Shu Zhang, Ravi Ganti, Krisztian Balog, Filip Radlinski, Fernando Pereira, and Arun Tejasvi Chaganty. Talk the walk: synthetic data generation for conversational music recommendation. <em>arXiv preprint arXiv:2301.11489</em>, 2023.</p>
</div>
<div class="citation" id="id216" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LGW+23<span class="fn-bracket">]</span></span>
<p>Mark Levy, Bruno Di Giorgi, Floris Weers, Angelos Katharopoulos, and Tom Nickson. Controllable music production with diffusion models and guidance gradients. In <em>Diffusion Models Workshop at NeurIPS</em>. 2023.</p>
</div>
<div class="citation" id="id312" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LTL24<span class="fn-bracket">]</span></span>
<p>Dongting Li, Chenchong Tang, and Han Liu. Audio-llm: activating theâ capabilities ofâ large language models toâ comprehend audio data. In Xinyi Le and Zhijun Zhang, editors, <em>Advances in Neural Networks – ISNN 2024</em>, 133–142. Singapore, 2024. Springer Nature Singapore.</p>
</div>
<div class="citation" id="id76" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LCY+23a<span class="fn-bracket">]</span></span>
<p>Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. AudioLDM: text-to-audio generation with latent diffusion models. In <em>International Conference on Machine Learning (ICML)</em>. 2023.</p>
</div>
<div class="citation" id="id291" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LCY+23b<span class="fn-bracket">]</span></span>
<p>Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo P. Mandic, Wenwu Wang, and Mark D. Plumbley. Audioldm: text-to-audio generation with latent diffusion models. In <em>International Conference on Machine Learning, ICML</em>, volume 202 of Proceedings of Machine Learning Research, 21450–21474. PMLR, 2023.</p>
</div>
<div class="citation" id="id77" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LYL+24<span class="fn-bracket">]</span></span>
<p>Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. Audioldm 2: learning holistic audio generation with self-supervised pretraining. <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2024.</p>
</div>
<div class="citation" id="id55" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LHSS24<span class="fn-bracket">]</span></span>
<p>Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, and Ying Shan. Music understanding llama: advancing text-to-music generation with question answering and captioning. In <em>ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, volume, 286–290. 2024. <a class="reference external" href="https://doi.org/10.1109/ICASSP48485.2024.10447027">doi:10.1109/ICASSP48485.2024.10447027</a>.</p>
</div>
<div class="citation" id="id32" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Liu19<span class="fn-bracket">]</span></span>
<p>Yinhan Liu. Roberta: a robustly optimized bert pretraining approach. <em>arXiv preprint arXiv:1907.11692</em>, 2019.</p>
</div>
<div class="citation" id="id22" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MBQF21<span class="fn-bracket">]</span></span>
<p>Ilaria Manco, Emmanouil Benetos, Elio Quinton, and György Fazekas. Muscaps: generating captions for music audio. In <em>2021 International Joint Conference on Neural Networks (IJCNN)</em>, 1–8. IEEE, 2021.</p>
</div>
<div class="citation" id="id25" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MBQF22a<span class="fn-bracket">]</span></span>
<p>Ilaria Manco, Emmanouil Benetos, Elio Quinton, and György Fazekas. Contrastive audio-language learning for music. <em>arXiv preprint arXiv:2208.12208</em>, 2022.</p>
</div>
<div class="citation" id="id14" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MBQF22b<span class="fn-bracket">]</span></span>
<p>Ilaria Manco, Emmanouil Benetos, Elio Quinton, and György Fazekas. Learning music audio representations via weak language supervision. In <em>ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 456–460. IEEE, 2022.</p>
</div>
<div class="citation" id="id6" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MSN24<span class="fn-bracket">]</span></span>
<p>Ilaria Manco, Justin Salamon, and Oriol Nieto. Augment, drop &amp; swap: improving diversity in llm captions for efficient music-text representation learning. <em>arXiv preprint arXiv:2409.11498</em>, 2024.</p>
</div>
<div class="citation" id="id302" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MWD+23<span class="fn-bracket">]</span></span>
<p>Ilaria Manco, Benno Weck, Seungheon Doh, Minz Won, Yixiao Zhang, Dmitry Bodganov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, and others. The song describer dataset: a corpus of audio captions for music-and-language evaluation. <em>arXiv preprint arXiv:2311.10057</em>, 2023.</p>
</div>
<div class="citation" id="id72" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MM24<span class="fn-bracket">]</span></span>
<p>Hila Manor and Tomer Michaeli. Zero-shot unsupervised and text-based audio editing using ddpm inversion. <em>International Conference on Machine Learning (ICML)</em>, 2024.</p>
</div>
<div class="citation" id="id80" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MTP+23<span class="fn-bracket">]</span></span>
<p>Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele Mancusi, Luca Cosmo, and Emanuele Rodolà. Multi-source diffusion models for simultaneous music generation and separation. <em>arXiv preprint arXiv:2302.02257</em>, 2023.</p>
</div>
<div class="citation" id="id47" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MSSR23<span class="fn-bracket">]</span></span>
<p>Daniel McKee, Justin Salamon, Josef Sivic, and Bryan Russell. Language-guided music recommendation for video via prompt analogies. In <em>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, volume, 14784–14793. 2023. <a class="reference external" href="https://doi.org/10.1109/CVPR52729.2023.01420">doi:10.1109/CVPR52729.2023.01420</a>.</p>
</div>
<div class="citation" id="id36" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MKG+16<span class="fn-bracket">]</span></span>
<p>Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and Yoshua Bengio. Samplernn: an unconditional end-to-end neural audio generation model. <em>arXiv preprint arXiv:1612.07837</em>, 2016.</p>
</div>
<div class="citation" id="id287" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MKG+17<span class="fn-bracket">]</span></span>
<p>Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron C. Courville, and Yoshua Bengio. Samplernn: an unconditional end-to-end neural audio generation model. In <em>International Conference on Learning Representations, ICLR</em>. OpenReview.net, 2017.</p>
</div>
<div class="citation" id="id257" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MGG+23<span class="fn-bracket">]</span></span>
<p>Jan Melechovsky, Zixun Guo, Deepanway Ghosal, Navonil Majumder, Dorien Herremans, and Soujanya Poria. Mustango: toward controllable text-to-music generation. 2023.</p>
</div>
<div class="citation" id="id284" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MJXZ23<span class="fn-bracket">]</span></span>
<p>Lejun Min, Junyan Jiang, Gus Xia, and Jingwei Zhao. Polyffusion: A diffusion model for polyphonic score generation with internal and external controls. In <em>Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR</em>, 231–238. 2023.</p>
</div>
<div class="citation" id="id82" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MWPT19<span class="fn-bracket">]</span></span>
<p>Noam Mor, Lior Wolf, Adam Polyak, and Yaniv Taigman. A universal music translation network. In <em>International Conference on Learning Representations (ICLR)</em>. 2019.</p>
</div>
<div class="citation" id="id42" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NCL+18<span class="fn-bracket">]</span></span>
<p>Juhan Nam, Keunwoo Choi, Jongpil Lee, Szu-Yu Chou, and Yi-Hsuan Yang. Deep learning for audio-based music classification and tagging: teaching computers to distinguish rock from bach. <em>IEEE signal processing magazine</em>, 2018.</p>
</div>
<div class="citation" id="id81" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NPA+24<span class="fn-bracket">]</span></span>
<p>Javier Nistal, Marco Pasini, Cyran Aouameur, Maarten Grachten, and Stefan Lattner. Diff-a-riff: musical accompaniment co-creation via latent diffusion models. <em>arXiv:2406.08384</em>, 2024.</p>
</div>
<div class="citation" id="id253" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NMBKB24a<span class="fn-bracket">]</span></span>
<p>Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J. Bryan. DITTO-2: distilled diffusion inference-time t-optimization for music generation. In <em>International Society for Music Information Retrieval (ISMIR)</em>. 2024.</p>
</div>
<div class="citation" id="id241" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NMBKB24b<span class="fn-bracket">]</span></span>
<p>Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J. Bryan. DITTO: Diffusion inference-time T-optimization for music generation. In <em>International Conference on Machine Learning (ICML)</em>. 2024.</p>
</div>
<div class="citation" id="id155" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NZC+24<span class="fn-bracket">]</span></span>
<p>Zachary Novack, Ge Zhu, Jonah Casebeer, Julian McAuley, Taylor Berg-Kirkpatrick, and Nicholas J. Bryan. Presto! distilling steps and layers for accelerating music generation. In <em>N/A</em>. 2024. URL: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:273186269">https://api.semanticscholar.org/CorpusID:273186269</a>.</p>
</div>
<div class="citation" id="id211" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>OLV18<span class="fn-bracket">]</span></span>
<p>Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. <em>arXiv:1807.03748</em>, 2018.</p>
</div>
<div class="citation" id="id13" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>OWJ+22<span class="fn-bracket">]</span></span>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and others. Training language models to follow instructions with human feedback. <em>Advances in neural information processing systems</em>, 2022.</p>
</div>
<div class="citation" id="id120" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PSK+24<span class="fn-bracket">]</span></span>
<p>Julian D Parker, Janne Spijkervet, Katerina Kosta, Furkan Yesiler, Boris Kuznetsov, Ju-Chiang Wang, Matt Avent, Jitong Chen, and Duc Le. Stemgen: a music generation model that listens. In <em>ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 1116–1120. IEEE, 2024.</p>
</div>
<div class="citation" id="id88" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PX23<span class="fn-bracket">]</span></span>
<p>William Peebles and Saining Xie. Scalable diffusion models with transformers. In <em>IEEE/CVF International Conference on Computer Visio (ICCV)</em>. 2023.</p>
</div>
<div class="citation" id="id45" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PHBD03<span class="fn-bracket">]</span></span>
<p>Geoffroy Peeters Perfecto Herrera-Boyer and Shlomo Dubnov. Automatic classification of musical instrument sounds. <em>Journal of New Music Research</em>, 32(1):3–21, 2003. <a class="reference external" href="https://doi.org/10.1076/jnmr.32.1.3.16798">doi:10.1076/jnmr.32.1.3.16798</a>.</p>
</div>
<div class="citation" id="id30" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RKH+21<span class="fn-bracket">]</span></span>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and others. Learning transferable visual models from natural language supervision. In <em>International conference on machine learning</em>, 8748–8763. PMLR, 2021.</p>
</div>
<div class="citation" id="id34" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RWC+19<span class="fn-bracket">]</span></span>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and others. Language models are unsupervised multitask learners. <em>OpenAI blog</em>, 2019.</p>
</div>
<div class="citation" id="id31" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RSR+20<span class="fn-bracket">]</span></span>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. <em>Journal of machine learning research</em>, 21(140):1–67, 2020.</p>
</div>
<div class="citation" id="id278" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RER+18<span class="fn-bracket">]</span></span>
<p>Adam Roberts, Jesse H. Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck. A hierarchical latent vector model for learning long-term structure in music. In <em>Proceedings of the 35th International Conference on Machine Learning, ICML</em>, volume 80 of Proceedings of Machine Learning Research, 4361–4370. PMLR, 2018.</p>
</div>
<div class="citation" id="id272" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RWD97<span class="fn-bracket">]</span></span>
<p><strong>missing journal in rovan1997igms</strong></p>
</div>
<div class="citation" id="id297" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SGZ+16<span class="fn-bracket">]</span></span>
<p>Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In <em>Neural Information Processing Systems NeurIPS</em>, 2226–2234. 2016.</p>
</div>
<div class="citation" id="id273" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SO17<span class="fn-bracket">]</span></span>
<p>Ian Simon and Sageev Oore. Performance rnn: generating music with expressive timing and dynamics. 2017.</p>
</div>
<div class="citation" id="id244" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SSDK+21<span class="fn-bracket">]</span></span>
<p>Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In <em>International Conference on Learning Representations</em>. 2021.</p>
</div>
<div class="citation" id="id39" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SLC07<span class="fn-bracket">]</span></span>
<p>Mohamed Sordo, Cyril Laurier, and Oscar Celma. Annotating music collections: how content-based similarity helps to propagate labels. In <em>ISMIR</em>, 531–534. 2007.</p>
</div>
<div class="citation" id="id269" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SCDBK24<span class="fn-bracket">]</span></span>
<p>Nikita Srivatsan, Ke Chen, Shlomo Dubnov, and Taylor Berg-Kirkpatrick. Retrieval guided music captioning via multimodal prefixes. In Kate Larson, editor, <em>Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24</em>, 7762–7770. International Joint Conferences on Artificial Intelligence Organization, 8 2024. AI, Arts &amp; Creativity. URL: <a class="reference external" href="https://doi.org/10.24963/ijcai.2024/859">https://doi.org/10.24963/ijcai.2024/859</a>, <a class="reference external" href="https://doi.org/10.24963/ijcai.2024/859">doi:10.24963/ijcai.2024/859</a>.</p>
</div>
<div class="citation" id="id252" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TZG+24<span class="fn-bracket">]</span></span>
<p>Or Tal, Alon Ziv, Itai Gat, Felix Kreuk, and Yossi Adi. Joint audio and symbolic conditioning for temporally controlled text-to-music generation. <em>arXiv:2406.10970</em>, 2024.</p>
</div>
<div class="citation" id="id60" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TYS+23<span class="fn-bracket">]</span></span>
<p>Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. SALMONN: Towards Generic Hearing Abilities for Large Language Models. In <em>The Twelfth International Conference on Learning Representations</em>. October 2023. URL: <a class="reference external" href="https://openreview.net/forum?id=14rn7HpKVk">https://openreview.net/forum?id=14rn7HpKVk</a> (visited on 2024-02-22).</p>
</div>
<div class="citation" id="id285" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>THDL24<span class="fn-bracket">]</span></span>
<p>John Thickstun, David Leo Wright Hall, Chris Donahue, and Percy Liang. Anticipatory music transformer. <em>Trans. Mach. Learn. Res.</em>, 2024. URL: <a class="reference external" href="https://openreview.net/forum?id=EBNJ33Fcrl">https://openreview.net/forum?id=EBNJ33Fcrl</a>.</p>
</div>
<div class="citation" id="id271" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TL89<span class="fn-bracket">]</span></span>
<p>Peter M. Todd and Gareth Loy. A connectionist approach to algorithmic composition. <em>Computer Music Journal</em>, 13:173–194, 1989.</p>
</div>
<div class="citation" id="id16" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TBTL08<span class="fn-bracket">]</span></span>
<p>Douglas Turnbull, Luke Barrington, David Torres, and Gert Lanckriet. Semantic annotation and retrieval of music and sound effects. <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, 16(2):467–476, 2008.</p>
</div>
<div class="citation" id="id43" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TC02<span class="fn-bracket">]</span></span>
<p>G. Tzanetakis and P. Cook. Musical genre classification of audio signals. <em>IEEE Transactions on Speech and Audio Processing</em>, 10(5):293–302, 2002. <a class="reference external" href="https://doi.org/10.1109/TSA.2002.800560">doi:10.1109/TSA.2002.800560</a>.</p>
</div>
<div class="citation" id="id10" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Uni01<span class="fn-bracket">]</span></span>
<p>International Telecommunication Union. <em>Method for the Subjective Assessment of Intermediate Quality Level of Audio Systems</em>. ITU-R Recommendation BS.1534-1, 2001.</p>
</div>
<div class="citation" id="id37" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>VDODZ+16<span class="fn-bracket">]</span></span>
<p>Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, and others. Wavenet: a generative model for raw audio. <em>arXiv preprint arXiv:1609.03499</em>, 2016.</p>
</div>
<div class="citation" id="id286" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>vdODZ+16<span class="fn-bracket">]</span></span>
<p>Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In <em>Speech Synthesis Workshop, SSW</em>, 125. ISCA, 2016.</p>
</div>
<div class="citation" id="id190" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>VSP+17<span class="fn-bracket">]</span></span>
<p>Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In <em>Neural Information Processing Systems (NeurIPS)</em>. 2017.</p>
</div>
<div class="citation" id="id308" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WZL+24<span class="fn-bracket">]</span></span>
<p>Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, and Nancy F Chen. Audiobench: a universal benchmark for audio large language models. <em>arXiv preprint arXiv:2406.16020</em>, 2024.</p>
</div>
<div class="citation" id="id281" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WZZ+20<span class="fn-bracket">]</span></span>
<p>Ziyu Wang, Yiyi Zhang, Yixiao Zhang, Junyan Jiang, Ruihan Yang, Gus Xia, and Junbo Zhao. PIANOTREE VAE: structured representation learning for polyphonic music. In <em>Proceedings of the 21th International Society for Music Information Retrieval Conference, ISMIR</em>, 368–375. 2020.</p>
</div>
<div class="citation" id="id7" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WKGS24<span class="fn-bracket">]</span></span>
<p>Benno Weck, Holger Kirchhoff, Peter Grosche, and Xavier Serra. Wikimute: a web-sourced dataset of semantic descriptions for music audio. In <em>International Conference on Multimedia Modeling</em>, 42–56. Springer, 2024.</p>
</div>
<div class="citation" id="id58" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WMB+24<span class="fn-bracket">]</span></span>
<p>Benno Weck, Ilaria Manco, Emmanouil Benetos, Elio Quinton, George Fazekas, and Dmitry Bogdanov. MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models. In <em>25th International Society for Music Information Retrieval Conference</em>. August 2024. arXiv:2408.01337 [cs, eess]. URL: <a class="reference external" href="http://arxiv.org/abs/2408.01337">http://arxiv.org/abs/2408.01337</a> (visited on 2024-08-21), <a class="reference external" href="https://doi.org/10.48550/arXiv.2408.01337">doi:10.48550/arXiv.2408.01337</a>.</p>
</div>
<div class="citation" id="id11" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WBZ+21<span class="fn-bracket">]</span></span>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. <em>arXiv preprint arXiv:2109.01652</em>, 2021.</p>
</div>
<div class="citation" id="id8" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Whi05<span class="fn-bracket">]</span></span>
<p>Brian Alexander Whitman. <em>Learning the meaning of music</em>. PhD thesis, Massachusetts Institute of Technology, 2005.</p>
</div>
<div class="citation" id="id53" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WCS21<span class="fn-bracket">]</span></span>
<p>Minz Won, Keunwoo Choi, and Xavier Serra. Semi-supervised music tagging transformer. In <em>Proc. of International Society for Music Information Retrieval</em>. 2021.</p>
</div>
<div class="citation" id="id52" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WON+21<span class="fn-bracket">]</span></span>
<p>Minz Won, Sergio Oramas, Oriol Nieto, Fabien Gouyon, and Xavier Serra. Multimodal metric learning for tag-based music retrieval. In <em>ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 591–595. IEEE, 2021.</p>
</div>
<div class="citation" id="id63" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WNN+24<span class="fn-bracket">]</span></span>
<p>Junda Wu, Zachary Novack, Amit Namburi, Jiaheng Dai, Hao-Wen Dong, Zhouhang Xie, Carol Chen, and Julian McAuley. Futga: towards fine-grained music understanding through temporally-enhanced generative augmentation. <em>arXiv preprint arXiv:2407.20445</em>, 2024.</p>
</div>
<div class="citation" id="id18" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WDWB23<span class="fn-bracket">]</span></span>
<p>Shih-Lun Wu, Chris Donahue, Shinji Watanabe, and Nicholas J Bryan. Music controlnet: multiple time-varying controls for music generation. <em>arXiv preprint arXiv:2311.07069</em>, 2023.</p>
</div>
<div class="citation" id="id205" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WCZ+23<span class="fn-bracket">]</span></span>
<p>Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In <em>IEEE International Conference on Audio, Speech and Signal Processing (ICASSP)</em>. 2023.</p>
</div>
<div class="citation" id="id274" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>YCY17<span class="fn-bracket">]</span></span>
<p>Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang. Midinet: A convolutional generative adversarial network for symbolic-domain music generation. In <em>Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR</em>, 324–331. 2017.</p>
</div>
<div class="citation" id="id309" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>YXL+24<span class="fn-bracket">]</span></span>
<p>Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, and Jingren Zhou. AIR-bench: benchmarking large audio-language models via generative comprehension. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, <em>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 1979–1998. Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/2024.acl-long.109">https://aclanthology.org/2024.acl-long.109</a>, <a class="reference external" href="https://doi.org/10.18653/v1/2024.acl-long.109">doi:10.18653/v1/2024.acl-long.109</a>.</p>
</div>
<div class="citation" id="id29" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>YWV+22<span class="fn-bracket">]</span></span>
<p>Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: contrastive captioners are image-text foundation models. <em>arXiv preprint arXiv:2205.01917</em>, 2022.</p>
</div>
<div class="citation" id="id310" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZDY+24<span class="fn-bracket">]</span></span>
<p>Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yu-Gang Jiang, and Xipeng Qiu. AnyGPT: unified multimodal LLM with discrete sequence modeling. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, <em>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 9637–9662. Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/2024.acl-long.521">https://aclanthology.org/2024.acl-long.521</a>, <a class="reference external" href="https://doi.org/10.18653/v1/2024.acl-long.521">doi:10.18653/v1/2024.acl-long.521</a>.</p>
</div>
<div class="citation" id="id85" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZIX+24<span class="fn-bracket">]</span></span>
<p>Yixiao Zhang, Yukara Ikemiya, Gus Xia, Naoki Murata, Marco A Martínez-Ramírez, Wei-Hsiang Liao, Yuki Mitsufuji, and Simon Dixon. Musicmagus: zero-shot text-to-music editing via diffusion models. <em>arXiv preprint arXiv:2402.06178</em>, 2024.</p>
</div>
<div class="citation" id="id311" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZZM+24a<span class="fn-bracket">]</span></span>
<p>Mengjie Zhao, Zhi Zhong, Zhuoyuan Mao, Shiqi Yang, Wei-Hsiang Liao, Shusuke Takahashi, Hiromi Wakaki, and Yuki Mitsufuji. Openmu: your swiss army knife for music understanding. <em>arXiv preprint arXiv:2410.15573</em>, 2024.</p>
</div>
<div class="citation" id="id306" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZZM+24b<span class="fn-bracket">]</span></span>
<p>Mengjie Zhao, Zhi Zhong, Zhuoyuan Mao, Shiqi Yang, Wei-Hsiang Liao, Shusuke Takahashi, Hiromi Wakaki, and Yuki Mitsufuji. OpenMU: Your Swiss Army Knife for Music Understanding. October 2024. arXiv:2410.15573. URL: <a class="reference external" href="http://arxiv.org/abs/2410.15573">http://arxiv.org/abs/2410.15573</a> (visited on 2024-11-09).</p>
</div>
<div class="citation" id="id259" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZCDB24<span class="fn-bracket">]</span></span>
<p>Ge Zhu, Juan-Pablo Caceres, Zhiyao Duan, and Nicholas J. Bryan. MusicHiFi: fast high-fidelity stereo vocoding. <em>IEEE Signal Processing Letters (SPL)</em>, 2024.</p>
</div>
<div class="citation" id="id189" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZWCD23<span class="fn-bracket">]</span></span>
<p>Ge Zhu, Yutong Wen, Marc-André Carbonneau, and Zhiyao Duan. Edmsound: spectrogram based diffusion models for efficient and high-quality audio synthesis. <em>arXiv:2311.08667</em>, 2023.</p>
</div>
</div>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="conclusion/beyondtext.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">超越基于文本的交互</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Seung Heon Doh, Ilaria Manco, Zachary Novack, Jong Wook Kim, Ke Chen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>